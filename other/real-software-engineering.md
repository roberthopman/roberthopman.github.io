---
layout: page
title:  "Software Art Thou: Glenn Vanderburg - Real Software Engineering"
---

Source: [https://www.youtube.com/watch?v=RhdlBHHimeM](https://www.youtube.com/watch?v=RhdlBHHimeM)

Note: Transcript from YouTube given to ChatGPT4 to add dots, commas, quotes and line breaks.

<hr>

Good evening. I am here to tell you where software engineering went wrong and what we can and should do to fix it and breathe new life into it. I like to keep my goals modest. Every few months, it seems we have this argument in our field: are we or are we not engineers? Is what we do engineering, or isn't it? Sometimes, the players, even on the anti-engineering side, are people who had really deep, fundamental contributions in the early days of software engineering, like Tom DeMarco.

My friend Dave Thomas, one of the co-authors of the pragmatic programmer, once wrote this: "Calling what we do engineering doesn't make us engineers. It makes us liars." Now, I don't think Dave would affirm this statement today. He wrote this on the extreme programming mailing list about 15 years ago, but it's one of the things, one of two things, that he said around that time. It was about the time when he was involved in writing the agile manifesto. It was one of two things he said around that time that really started the line of thinking in my mind that, many years later, led to this talk. So, I'm grateful to him for that.

These "software isn't engineering" arguments usually boil down to one of two things: either software development doesn't look like what I think engineering looks like, or software development feels more creative to me. It's more like an art or a craft or something else like that. It doesn't feel like engineering. Notice that those are really two ways of saying the same thing: I have in my mind an idea of what engineering is like, and software engineering either feels different or looks different, or the way people do it seems different, so I don't think it's the same thing. And yet, we keep calling ourselves engineers. We do it in job postings and job titles and org charts.

Jeff, I don't know where he went, but Jeff's title is software, it's senior software engineer or something like that, right? How did we get to this point, right? How did we get to this weird state where we can't even agree on what it is that we fundamentally do? There's this interesting cognitive dissonance in our field, and I'd like to try to get to the bottom of that.

Where did it start? Software development first began to be seriously talked about as engineering in 1968 in Garmisch, Germany, at a software engineering conference promoted and paid for by NATO of all things. And then a year later, there was a follow-up conference in Rome. It was in Rome, in 1969, that software engineering was really launched as an active academic pursuit. The first conference, if you read the proceedings, the first conference was characterized by debate, by a certain kind of intellectual humility about how much they didn't yet know. The second conference, a year later, was characterized by a lot of certainty, and I would say, and some of the people involved in that conference would say, that it was premature certainty. People were pretending to know more than they really did.

Academic software engineering, which is what I'm going to call that kind of academic field that branched off from there, I'll sometimes refer to it as ASE for short. Academic software engineering aimed at becoming like engineering. "We must be more like engineers." That meant requirements, specifications, analysis, design, test, was very formal, very analytical. Correctness came through eliminating errors early in the process before code was written, through a series of inspections and validations of those documents and specifications. It was striving to be like engineers. And you know, think of the stereotype of what engineers do: building a bridge. They don't go out and start, you know, mortaring stones together. They sit down and draw plans and blueprints and validate those, and iterate on this design, and they make a blueprint, and then they give it to laborers who do the construction. So, we should be like that.

So, that's where it started. And I'm gonna real quickly try to give you a big picture view of the 50 years since then before we dive into the details. At that time, in 1969, mainstream software development really didn't have any kind of unified view of itself. It didn't have a goal in mind for where it hoped to be. People were just writing software as best they could, often not very well. Projects had a really high failure rate in those days. There were a lot of people exploring better ways of doing individual little pieces of the puzzle: better algorithms, better programming languages, better machine architectures, all kinds of different things. But no real overarching view of what the field should look like.

And academic software engineering kind of set a target of this idealized view of engineering, and they set off in that direction. And they had some success with this. They defined some rationale, and that's a technical term, we'll come back to later, we'll come back to a lot of things later. They had, they defined some rational processes for software development. Waterfall is one example, but not by any means the only one, although they all had that same kind of spirit of phases.

But academic software engineering fell short of its goal. Those processes were very costly and time-consuming, and often not really any more reliable than the old way. Mainstream software engineering, on the other hand, in the meantime, continued more or less as it was, and didn't pay much attention to the academic software engineering results. Some large companies tried to follow these processes. I call it academic software engineering because it mostly didn't escape from academia and ever get adopted by industry, but some large companies who could afford to gave it a really serious try. And despite the high cost, they still had a very high failure rate.

Alastair Coburn, in the mid-90s, spent a lot of time interviewing participants on projects that had tried these things and other processes, and he had a really interesting set of three questions that he would ask these project participants. The first one was, "Was your project a success?" Most of the time, but not all the time, he got fairly good, clear yes or no answers for that, but sometimes it was a little more vague. The second question was, "How strictly did you adhere to the process that you chose for this project?" Interestingly, he got very different answers from the project managers and leaders of the project than he did from the programmers in the trenches, as it were. Often, the managers thought they were originally adhering to the process, but in fact, they weren't. And the third question, which is the genius one, is, "Would you willingly choose this process again for a new project?" Very often, the answer was no.

From the answers to these questions, Alastair built a very compelling case that in those days, even the projects that succeeded using the ASC project processes, succeeded more in spite of the processes than because of them. They succeeded because sharp people who cared were willing to circumvent the process and short-circuited in some in order to get things done. So now, meanwhile, mainstream development carried on, not paying much attention to this. And this led to a strange situation where we had a field where the term engineering was reserved for a set of practices that everybody knew didn't work. That's weird, because in every other field, that term is reserved for practices that people have demonstrated work pretty well.

And this is partly what led to the divide we see today, where people, a lot of people are just, "software development is incompatible with engineering; don't even try." Well, in the mid-90s, as a reaction to this weird situation, a number of people started trying to find other, more appropriate, disciplined ways of building software, ways to do a better job, have a higher success rate, meet customers' needs more reliably, but in a way that didn't cost so much, it was more appropriate for our field. And that was the agile movement, and it really solidified in 2001 with the publication of the agile manifesto.

Where academic software engineering was very analytical, agile, on the other hand, was iterative and experimental. All of the phases, or activities, were performed iteratively and in concert with each other. Correctness was achieved by feedback and testing over time. The mainstream has bent towards the agile methods with increasing success, and the academic software engineering has continued, but as its influence has waned.

So, that has led us to this situation where we have two views. One is, "software development should be more like engineering," and the other is, "software development simply isn't like engineering, so we should stop trying." And my view is that both of these views are mistaken, because they are both based on the same core misunderstanding. That misunderstanding was that idealized view of engineering that academic software engineering started out in pursuit of.

In the same year that the Rome conference happened, in 1969, Herbert Simon published a book called "The Sciences of the Artificial," and in it, he defined a rational model of decision-making. That's that technical term I was talking about. And in 1986, in a paper he wrote called "A Rational Design Process: How and Why to Fake It," David Parnas, an important software engineering researcher, described how that model maps onto software development, how the rational model of decision-making maps onto software development. And this is a little tongue-in-cheek, but only a little.
Parnas wrote that if you were pursuing the rational model, you would first establish and document requirements, design and document the module structure, design and document the module interfaces, design a document the uses hierarchy, design a document the module internal structures, write programs, and maintain. Again, a little tongue-in-cheek, but it's not far off from the idealized view of engineering that academic software engineering started with. Both sides of the arguments we're having today generally accept that the rational model accurately describes the engineering ideal. They differ in terms of whether they think we should or even can try to emulate that in the software world. My view is different. I claim that software development already is an engineering field; it always has been. I mean, think about it, we are seeking and developing disciplined, reliable, cost-effective ways of building useful things for people at large scale. What else could it possibly be?

Interestingly, other engineering disciplines really haven't had the kind of existential crisis about whether they're engineers or not. They just get down to engineering, sometimes badly, and have to learn hard lessons, but they don't worry about whether they're engineering or not. The rational model, as it turns out, is a caricature of engineering. No branch of engineering, even the most established and analytical and formal of them, really works that way, and some established branches of engineering don't look like that at all.

We don't need to become engineers; what we need to do is to continue to refine our own engineering practices and discipline, as all engineers are always doing. But accepting that is a lot to ask of you. I'm basically saying that an entire large, heavily funded academic field went down the wrong path from day one and continued down that path for 30 years, at least. The least I can do is to try to back that claim up, and so the rest of the talk is, we'll dive into the mistakes that analytical software engineering or academic software engineering made. We'll talk about what engineering really is like in the other disciplines, finally look at what software engineering is in its best form today, and then look a little bit at what's next.

So, where did academic software engineering go wrong? I don't know why I said that it is analytical and formal. You'll see where that word comes from in a minute. First of all, there were objections from the beginning that were ignored. This is Brian Randall. Randall is a noted professor of computer science, now professor emeritus at Newcastle University in the UK. He was the assistant editor of the conference proceedings at the first conference in Garmisch, Germany, and he was the lead editor of those proceedings in Rome. About 25 years later, in 1996, at the history of software engineering conference, he presented a memoir called the 1968-69 NATO software engineering reports, and I'm going to show you and read for you a lengthy quote from that memoir. He wrote, "Unlike the first conference, at which it was fully accepted that the term software engineering expressed a need rather than a reality, in Rome, there was already a slight tendency to talk as if the subject already existed, and it became clear during the conference that the organizers had a hidden agenda, namely that of persuading NATO to fund the setting up of an international software engineering Institute. However, things did not go according to their plan. The discussion sessions, which were meant to provide evidence of strong and extensive support for this proposal, were instead marked by considerable skepticism and led one of the participants, Tom Simpson of IBM, to write a splendid short satire on masterpiece engineering, which you can Google for and read, and I recommend that you do. It was little surprise to any of the participants in their own conference that no attempt was made to continue the NATO conference series, but the software engineering bandwagon began to roll as many people started to use the term to describe their work, to my mind, often with very little justification. I made a particular point for many years of refusing to use the term software engineering or to be associated with any event that used it. I won't say any more about that, but academic software engineering ignored important dissenting opinions from the early days."

This is a quote from the Rome conference, no, from the Garmisch conference, the first one, by Alan Perlis, a noted computer scientist from the early days, who wanted to voice his strong opinion about the way things should go. And before I read these three points that he made, I'll say they used different terms then, and this is not phrased in the way that we would phrase it today. But I want you to see if you can figure out what he's describing here: "A software system can best be designed if the testing is interlaced with the designing, instead of being used after the design. A simulation which matches the requirements contains the control which organizes the design of the system through successive repetitions of this process of interlaced testing and design. The model ultimately becomes the software system itself. In effect, the testing and the replacement of simulations with modules that are deeper and more detailed goes on with the simulation model controlling, as it were, the place and order in which these things are done." What is he describing here? What? Prototyping, test-driven development, iterative test-driven development, using mock services for parts of the system that haven't been written yet, right? 1968, and then in 1969, we went down a completely different path.

They began with the rational model and also a view of engineering as applied science. Fred Brooks, the author of the mythical man-month, also wrote another wonderful book that doesn't get as much attention as it deserves, called the design of design. And in the preface, he talks about the rational model a little bit. He says, "The rational model may seem naive, but it is a very natural model for people to conceive." Winston Royce made that same point in his paper that introduced the waterfall method. He said, "This is very obvious, and this is how people are doing it today," and he then went on for the rest of the paper, which nobody ever remembers, saying why it was a disaster to build software that way and he should never do it. "It is a natural model through people to conceive." Yet, from early on, there have been cogent critiques of the rational model from the design community. David Parnas, another I've already mentioned him as an important software engineering researcher, said, "The picture of the software designer deriving his design in the rational way from a statement of requirements is quite unrealistic. No system has ever been developed in that way, and probably none ever will." And I mentioned that software engineering started with the assumption of engineering as applied science. Also, in the preface of Design, Fred Brooks reminisced about studying engineering at Harvard in the 50s, and at the time, the Dean of Engineering at Harvard was John Van Vleck, who was a Nobel Prize-winning physicist, not an engineer by training. And Brooks had this to say, "Van Vleck was very concerned that the practice of engineering be put on a firmer scientific basis. He led a vigorous shift of American engineering education away from design, it had been seen as a design practice and discipline, toward Applied Science. The pendulum swung too far, reaction set in, and the teaching of design has been contentious ever since."

Another mistake they made was to devalue the role of code in the software engineering process. Devaluing the role of code. At that Rome conference, no, I keep getting confused, at the Garmisch conference in '68, Friedrich Bower said, "What is needed is not classical mathematics, but mathematics instead." In any case, systems should be built at levels and modules which form a mathematical constructor. And elaborating on this, Edsger Dijkstra said, "Our basic tools are mathematical in nature." This was in response to people talking about how engineers worked with equations and math and rules, and they proved things that way. And these two men were trying to get across the point that code and logic don't look like classical mathematics, but they are inherently mathematical, and there is a rigor to them that you can't escape. "We don't necessarily need to go to classical mathematics to achieve the rigor that engineering system disciplines desire." Another reflection of that, in 2007 at the Uppsala conference in Montreal, I heard David Parnas give a talk reflecting on his career in software engineering, and he said two things short distance apart in the talk that really struck me. First, he said, "In engineering, people design documentation," which is true. But a little later, he explicitly said that code can't fill that role for software, which is a serious error in my mind.

Finally, academic software engineering disproportionately emphasized correctness over cost. Their primary concern was how to build correct, reliable, robust systems. So, everybody who has any association with software engineering has seen the cost of change curve. This was based on research done by Barry Boehm and published in a book called Software Engineering Economics in 1981. And the idea is that the farther you go along in your project life cycle, the costlier it is to fix bugs, to correct errors. And so, the software engineering solution to this was, "Well, if it's cheaper to fix errors at the start of the project, let's do everything we can to find them as early as we can and fix, make all the change as early as we can in the project." But of course, doing this means extra care, extra inspections, extra reviews, and that has the secondary effect of increasing costs across the entire lifecycle, and organizations don't like that, so they pushed back down on that. And this is like stepping on a toothpaste tube, and it blows your schedule out. The cure was worse than the disease. We need to look for low-cost, high-impact processes instead of necessarily the right way that will eliminate all the errors. As an aside, all of the projects that Boehm studied for his original cost of change curve were waterfall projects. And so, decisions were made early in the process, but they didn't actually integrate and test until very late in the process. And so, what he was actually measuring wasn't the cost of change during the project lifecycle, but the cost of long feedback loops. His results actually show that the longer you wait after you make a decision before you validate it and correct it, the more it will cost, which really should push us towards tight iterations and short feedback loops.

So, those are the mistakes. What's real engineering like? Well, part of the problem is that engineers don't write about engineering. A chemical engineer might write about chemical engineering, or a mechanical engineer might write about mechanical engineering, but there's been very little written about engineering per se, as an umbrella over all the engineering disciplines. Bill Yvan Cohen, a mechanical engineering professor at the University of Texas, had this to say about the situation: "Unlike the extensive analysis of the scientific method, little significant research to date has sought the philosophical foundations of engineering. Library shelves groan under the weight of books by the most scholarly, most respected people analyzing the human activity called science. No equivalent body of research treats the engineering method." And that's mostly true. I've been looking for about 10 years, and so far, I've found eight books that even touch on what engineering is all about, independent of a particular discipline. Herbert Simon's The Science of the Artificial, Gordon Glaze's The Design of Design from 1969, which is a lovely little book, Definition of the Engineering Method by Bill Yvan Cohen, To Engineer is Human and other books by Henry Petroski, who's sort of the poet of engineering. But his first book, To Engineer is Human, that examines the role of failure in engineering design, really is the one that hits closest to that center that I was looking for. What Engineers Know and How They Know It by aerospace engineer Walter Vincenti, Engineering in the Mind's Eye by Eugene Ferguson, The Existential Pleasures of Engineering by Samuel Florman, and The Design of Design by Fred Brooks. And he acknowledges that his title was inspired by Gordon Clegg's earlier book.

Engineering is not like the caricature of the rational model. The clearest expression of the caricature I've ever seen came from a blog post by Bruce Eckel, where he wrote, "Programming is not some kind of engineering where all we have to do is put something in one end and turn the crank." Now, I don't want to be unfair to him. I use this quote a lot. He's a smart guy. I don't think he really thinks this is a description of engineering. It was a throwaway line on the way to a larger point he was making. But he said it, so I get to make fun of him for it. Let's be clear, there is no kind of engineering where all you have to do is put something in one end and turn the crank. Engineering is inherently a crude, creative, exploratory process, marked by false starts and dead ends and failures and learning processes, no matter what discipline you see. Bill Yvan Cohen describes it this way: "The engineering method is the use of heuristics to cause the best change in a poorly understood or uncertain situation within the available resources." That doesn't sound much like the caricature of engineering that we saw earlier, does it?

Another thing about engineering is that different engineering disciplines are very different. And to explain this, I'm going to use something called the process control model, where you can describe processes as existing on a continuum from defined at one end to impure at the other end. And the quotes that follow are from Schwaber and Beedle's original book about Scrum. And they employed the notion of process control model to define what Scrum should do. The defined process control model requires that every piece of work be completely understood.

A defined process can be started and allowed to run until completion with the same results every time. On the other hand, the empirical process control model provides and exercises control through frequent inspection and adaptation for processes that are imperfectly defined and generate unpredictable and unrepeatable outputs. The notion of the process control model originated with Chemical Engineering, where they don't actually design artifacts so much as processes that have to be controlled. What does that sound like? Interestingly, the chemical engineers that Schwaber and Sutherland learned this from were most amused when they learned that software engineering had chosen the defined end of the spectrum because they didn't think it was appropriate at all.

But to use this model to describe engineering processes themselves, the various engineering disciplines fall all along this spectrum. Civil engineering and structural engineering are more toward the defined end, but partly because they're older and more established, and have had longer to be refined. But also partly because it costs a lot more to go build a prototype and test the thing you're building, and so it's really helpful to be able to have formalisms and mathematical models and analytical methods to convince yourself of the correctness of your design before you go spend millions, or in some cases billions of dollars to go build it. Mechanical, aerospace, and chemical engineering exist somewhere in the middle. Electrical and industrial engineering are much more empirical. Industrial engineering designs systems that often have people as moving parts in them, so you can't really have a defined process there. And software engineering, I think, should be on the opposite end of the spectrum from civil and structural engineering, which is to say, on the opposite end of the spectrum from where academic software engineering first put it.

Another fact about engineering, which I've already alluded to, is that it is a creative pursuit. A couple of quotes to back this up from engineering is a creative pursuit. Eugene Ferguson, the conversion of an idea to an artifact which engages both the designer and the maker, is a complex and subtle process that will always be far closer to art than to science. Aerospace aeronautical engineer J.D. North, in a letter to the Royal Aeronautical Society in 1923, wrote this: "Airplanes are not designed by science but by art, in spite of some pretense and humbug to the contrary. There is a big gap between scientific research and the engineering product which has to be bridged by the art of the engineer." And in the preface to his original design of design, Cambridge engineering professor Gordon Glick, who was a mechanical engineering professor, thought it was so obvious that engineering was creative and artistic that he worried that readers might reject the basic principles he was attempting to layer on to the practice and wrote this: "At first sight, the idea of any rules or principles being superimposed on the creative mind seems more likely to hinder than to help, but this is really quite untrue in practice. Disciplined thinking focuses inspiration rather than blinkers it." Those are the views of experienced practicing engineers about mechanical, aeronautical, and structural engineering, that it is artistic and creative.

In engineering, math and formal methods can be employed, but they're just tools to get the job done. They're not magic sauce. I know in Australia, you don't have the tradition of newspaper comics that we do in America, or did when we still had newspapers, but I think you probably all know Calvin and Hobbes, right? So in my life as a parent, as a dad, I've tried to model myself on Calvin's dad, and that means, you know, when my kids had questions, I would invent ludicrously wrong answers to amuse myself. And this is one of my favorite Calvin and Hobbes cartoons about that: "How do they know the load limit on bridges, Dad?" And Dad says, just what I would say, "Well, they drive bigger and bigger trucks over the bridge until it breaks, then they weigh the last truck and rebuild the bridge. Simple." Why is this funny? Because it's ridiculous, it's also true.

How many of you have seen the video where Boeing engineers stress the wings of a Boeing 777 to the breaking point? Right, good. If you haven't seen it, go look it up; it's worth watching. They attach chains to the ends of the wings and two winches at the ceiling of this hangar, and they start cranking the wings up, and they do it until they break catastrophically. And then a very interesting thing happens: the engineers congratulate themselves. What are they happy about? What they learn. Something. What did they learn? When it broke where they expected, their math was right. They had mathematical models that told them where this would happen, but they weren't really sure until they tried it.

Recently, I've come up with another couple of interesting demonstrations of this. This is from Elon Musk's recent presentation about making humanity a multiplanetary species. So, let's see here, this is a further, their new rocket that they're designing. So, we tested it. We successfully tested it up to its design pressure, and then went a little further. So, we wanted to see where it would break, and we found out.

Another example of this: How many of you are familiar with the YouTube channel Smarter Every Day? A few. It's done by this American guy from Alabama. He's an engineer, and he does YouTube shows about interesting scientific or engineering things he's learned. And one thing he does, he has this super high-speed video camera, so that he can take a hundred thousand frames a second and do super slow-mo uses of things happening. And recently, he got in touch with somebody who makes sound suppressors, sometimes called silencers, for firearms. And the guy made some of his suppressors with an acrylic case, so that you could see the blast wave being baffled and deflected and dispersed inside the suppressor as the bullet went through. And I want to play this one little clip from that episode. This is called destructive testing, and it's my favorite part of engineering. Steve obviously knew this was going to break if we used a metal can, and this wouldn't have shattered. But what he did is, he weakened one part of the system, so that we could see how strong the other components were. Once you figure that out, you can figure out how to tweak your design, and you can make a really strong overall product without making it too bulky. This is really smart, and it's something that engineers do all the time, and it's one of my favorite things in engineering. Remember that when people tell you that software is an engineering because engineers have math to prove things, so they know it'll work because of the math, right?

Finally, one last example. In the early part of the 20th century, people were building suspension bridges all over the place. Quick trivia question: Who knows why people don't build suspension bridges very much anymore? Mm-hmm, well, because people got scared of them because a couple of them failed. And that's the same reason that about 30 years earlier, they stopped building cantilever bridges because some of them failed, and people got scared of them. And that's the same reason that 30 years earlier, they stopped building girder and truss bridges because, you know, so there was this bridge engineer from Hungary, I think, named Leon Moisseiff. He was working in the United States, and he was a consulting engineer on some major suspension bridge projects: Golden Gate, Mackinac Bridge in the north part of Michigan, and a couple of the bridges around Manhattan Island. And he was the leading proponent and developer of what at that time was the premier mathematical model for suspension bridges, which was called deflection theory. And you see, when you're building a billion-dollar structure, you can't just go build a bridge in a lab, you know, run trucks over it until it breaks, and weigh the last one. So, you need mathematical models to tell you what to do. But then, you know, the model isn't completely correct, so then you add a safety factor. You build it stronger than you think it really needs to be, by 50 percent, sometimes a hundred percent, to account for the inaccuracies in the model. Well, Moisseiff had a lot of confidence in deflection theory, and he knew that that safety factor cost money in materials and extra labor, and everything else. And it also made the bridges look kind of ugly to have it be thicker and bulkier than they needed to be. So, he got his own bridge project where he was made the principal engineer, and he decided to show off just what deflection theory could do. And that was the Tacoma Narrows Bridge, which tore itself apart in a mild wind. And it happened because deflection theory was tragically incomplete as a mathematical model. It accounted for downward stresses from gravity, from the weight of the bridge itself, and vehicles traveling over it. It accounted for sideways wind stresses. But up until that time, nobody had ever built a suspension bridge deck that was that thin relative to its length. And the Tacoma Narrows Bridge was subject to a force that nobody had ever seen in a suspension bridge before. When the deck is that thin, it can act as an airfoil, and there are also vertical stresses unevenly distributed across the deck, and that started a vibration in the deck which settled in on the resonant frequency of that bridge, and it was amplified by that, and eventually tore itself apart. So, remember this: engineers adopted formal methods to save money. The more accurate your model is, the less of a safety factor you have to build in, and that saves you money. And the fewer prototypes you have to build, if you're, for example, an aerospace engineer.
Ask your friendly neighborhood aerospace engineer how many formal modeling they would do if building a design of their prototype and testing it in a wind tunnel were instantaneous and free. You know, the answer is probably not none, but it's a lot less than they would do if they could just test it in a wind tunnel. Interestingly, now with fluid dynamics simulations, aerospace engineers are relying a lot more on simulated wind tunnel testing than they are on the traditional formal methods.

Eugene Ferguson said structural analyses, indeed any engineering calculations, must be employed with caution and judgment because mathematical models are always less complex than actual structures, processes, or machines. Finally, unlike academic software engineering, engineering disciplines typically are driven by practitioners. Innovation comes from the field and then is formalized and refined in academia, rather than originating in academia and going out to practitioners.

A few examples also exist. In the earlier XX century, a contemporary of Leon Wassouf was a Swiss bridge engineer named Robear Myart. Myart got interested in reinforced concrete, which was a relatively new material at the time. It had already been in use for bridges but kind of as a cheaper stone. Right, the bridge designs were the same as older stone bridges, but then they would just use reinforced concrete to build them. Myart realized that reinforced concrete had different properties than stone and so it could be used to build different kinds of structures. He had in mind lightweight, graceful structures like the Val She'll Bach and he built bridges like this throughout Switzerland. Interestingly, he did not have mathematics sophisticated enough to analyze and demonstrate the viability of these designs. He was soundly criticized by the civil engineering establishment in Europe for being a charlatan, endangering lives, cheating his customers by building bridges for them that would soon fall down, and so forth. Only one of Myart's bridges has ever fallen down in an avalanche, so, I think we can probably understand that.

What did he do? What do you think he did? He prototyped a little bit of trial and error. He had a good understanding of the physics and the physical principles, even if he didn't have the math to analyze them in detail. He built models at multiple scales and invited friends over to jump up and down on them, enrolled builders full of rocks over them, and things like that, and extrapolated from the behavior at multiple different scales to give him the confidence that the designs would be appropriate at full scale. Then he built them, and later the math caught up, demonstrated the viability of the designs, and these designs that he pioneered are standard parts of the bridge builder's toolkit today.

Another example: for some kinds of purposes, the optimal form of an arch is described by this equation, and it's not a parabola, or there's clothes. It's a form called a catenary, which you can try out yourself by hanging a chain from two points. That shape that a chain naturally falls into is a catenary, and you invert it, and it's the optimal form for freestanding arches. It uniformly spreads the stress. In a hanging chain example, it's tension, and in the arch, it's compression, but the same principle applies: that you uniformly distribute that stress over the components of the arch. Robert Hooke, the Renaissance scientist, demonstrated that this was optimal in 1671, but it wasn't until 20 years later, in 1691, that three other men, Leibniz, Huygens, and Bernoulli, independently found the equation that I showed back on the St. Louis Arch picture. But it was known to be a useful form for arches for and was used by engineers for many years before either those events happened.

The Cathedral Sagrada Familia in Barcelona, these are all pictures what I took myself. It was begun in 1882. It's projected to be finished in about ten years. It's a very unusual structure, and the designer, Antonin and Gaudí, wanted it to feel like you're walking through a forest as you're inside the structure, and he succeeded admirably. I want you to notice something about these arches, and especially the peak of the arch. What's at the very peak of an arch? Think of a stone arch, a keystone. The arches in the Sagrada Família have skylights, often very big skylights. How does this work? Well, he decided that he could do this if he designed the arches based not on the parabola, a parable, a parabola shape, but on elliptic geometry. I don't remember the names of the exact mathematical constructs, but no, not elliptic, hyperbolic, ellipses, hyperbolas. But in a familiar tale, he didn't have math sophisticated enough to really model these things and it results in really complex branching support structures. But even though he didn't have sophisticated enough math to prove these designs correct, you sure hope he got it correct because underneath the biggest open space in the middle of this thing, well, that's all supporting five giant towers that are going to go on top over the next 10 years. So, it better be right. Well, how did he do it? He exploited the catenary principle and built upside-down elaborate hanging string models. And he used a little weighted bags of sand tied to it to mimic the effects of extra load put on those portions of the structure and built the columns to match the shapes that the strings hung in. And he built a lot of these models, and he would hang muslin inside them and drape it in there so that he could stick a camera in and take pictures from the inside and then turn that upside down and, you know, see what it would look like on the outside and the inside. And later, he built sectional models of the more detailed sectional models of the Sagrada Família vaults, and they're absolutely magnificent. And again, in a recurring story, later the math caught up, and the current wave of engineers working on the structure have validated all of this with computer models and simulations, and it's all perfectly fine. So, engineering is not anything like the caricature, really.

So, what then is software engineering? This is the best definition of engineering I've been able to find of structural engineering. I want you to notice the tensions here. Structural engineering is the science and art of designing and making, with economy and elegance, structures so that they can safely resist the forces to which they may be subjected. There's always tension, there's always balance that needs to be resolved situationally, and by taste and care. So, based on that, I'm going to propose my definition of software engineering, which is the science and art of designing and making, with economy and elegance, systems so they can readily adapt to the situations to which they may be subjected. Based on that flawed idea of structural engineering analogy, what engineering is really like. Early software engineering processes were designed according to an analogy with traditional engineering disciplines, and the analogy goes something like this: In structural engineering, for example, you have an engineer, and that engineer works for a while and produces a design in document form that is then handed to laborers, which go build the structure. And so, by analogy, in software engineering, we should have engineers which produce a design in document form that they hand to laborers, which then build the resulting structure. But this analogy never really worked very well, this way of doing things never really worked very well, and we talked about that earlier. Part of the problem was that second step. We never figured out how to do that well enough and thoroughly enough so that the coders, in the third step, didn't have to finish the design and redesign and fix stuff, and everything. So, I don't know, maybe if the coders are going to be designing and redesigning and elaborating on the design and everything, maybe we blow that up and move them over there and say they're doing the engineering. Let's rebuild this analogy in a way that works. The other problem is, what are customers paying us for? See that stuff over on the right, the source code. Is that what our customers are paying us for? What are they paying us for? They're paying us for solutions, which, if the solution is software, is going to be running systems on real machines. What? Well, they're paying for what works for them, right? They don't care about that stuff. In actual fact, if we build this analogy properly, that is the design document. It's the detailed design document. I'm not saying it's the only design document, design overviews that guide you into the code and help you understand it are very helpful, but this is the detailed design document. If that's the design, then what corresponds to the laborers and construction workers? The computers. [Applause] DevOps, no, compilers, and language implementations, and they take that design and turn it into something that can actually solve the customer's problem. Now, notice, what's the most expensive part of the top row? The actual construction. That's the Construction cheapest part of the bottom row. So much of academic software engineering was designed around avoiding the construction phase until you knew everything was right. But if that's the cheapest part of our structure, why don't we just do it all the time? There's a word for that, but now we have continuous integration systems, right? That basically, every time you check in your new change to the code, this gets kicked off and it builds the Engineering Economics code, and that build consists of compilation but also running the tests and validating them.

Right, so we've got an upside-down engineering economics here, where the cheapest thing, the cheapest part of our process, is building and testing a prototype. And compared to the other engineers that work with atoms instead of bits, it's effectively instantaneous and free. So, the right way to design a software engineering process is not to avoid this step. The right way is to rely on it and do that and iterate, validating our designs with testing against the requirements.

We have this by the way, all comes, it's not my idea. It comes from a paper written in the early 90s by a man named Jack Reeves, called "What is Software Design?" He was writing that before automated unit testing was really a thing, and his thesis already makes sense even if you don't have the automated validation step in there. But adding automated unit testing and integration testing really makes it sing. Where's the model in this? Well, it's the code, right? Where's the document? It's the code, and our tools, remember what Dijkstra said, our tools are mathematical in nature, that's the math.

Okay, speaking of documents in math, we'll come back to David Parnas again. I keep referring to him, sometimes in good ways, this time it's in a bad way. This is an example, remember he said that code can't fill the role of documents in an engineering design. This is an example of something that he developed called tabular mathematical expressions as a way of specifying complex logic and requirements for code. I'm not going to expect you to understand that, so I have created a very simplified example describing division. You can understand that, right? By the way, do you see the mistake there? Oh wait, too late. This is another document that serves the same purpose. It describes by example the process of division, but this one has a twist. It's self-validating. It's a document that does work for us. You can run it and it can test the system. There are all kinds of different variations of this with different tools and I made this slide a few years ago, so there's new ones now. And we have documents that are like executable math. They do work for us and they can do validation for us, and we should embrace that and not reject it.

One last thing, how does a software process feel disciplined and rigorous if it doesn't look like the rational model? This diagram is the 12 original practices of extreme programming and the dependencies between them. It's actually 13 because I realized in doing this work that unit testing and acceptance testing were treated as one thing in the book, but they're really used for very different purposes and they're very different things and they have different dependencies. I built this diagram because Dave Thomas, the second comment he made that started me thinking about this stuff one day, he said, "You know, if you were a software developer and you built a system that was as tightly coupled as XP, I'd fire you." And I was like, he's right, except I liked that redundancy in extreme programming. It felt right to me, it felt good, and I had the suspicion that there's a hidden underlying structure to this that's simpler.

I built this diagram by reading there's a chapter in the original white extreme programming book by Kent Beck. The entire chapter is of the form, "You know this practice can't possibly work because it has shortcomings XY and Z, it's too lightweight, not rigorous enough." And that would be true except it's backfilled by these other practices that catch the things it doesn't. Right, and so I went through that chapter and drew all these lines and then I spent a lot of time in OmniGraffle dragging it around trying to make it look more coherent and figuring out some things. And I figured out that there are five practices that are just really kind of noise suppressors. They, like 40-hour week, right in one sense, that's not a practice, it just makes sure people are doing good work. It's increasing the input level of inputs into the system. And if you take those practices out, you get a set of practices that you can lay out on a continuum like this, related to the scale of decisions that they're giving you feedback about. In when your pair programming, you're thinking mostly about things at the level of statements and methods, and maybe a little bit of classes and interfaces in that level of design. Unit testing works at the level of methods and classes and interfaces, and continuous integration is making sure that all hangs together, and the design is reasonable, and they haven't got any errors in there that have made it past to that point, all the way on up to short releases, which is about, you know, periodically every few weeks, releasing the interim solution to your customer or a subset of your customers to let them validate that it's going the right direction and solving the problem that they want you to solve. And the other interesting thing is that you can replace the things on the left side of this arc with time scales, and these practices operate at increasingly large time scales as well. And pair programming, you're getting feedback from your pair on the level of seconds. In unit testing, you run those every few seconds or every minute and you get feedback at that level, all the way up to short releases, which is every six weeks or two months or something like that. So, extreme programming and agile processes in general are economical feedback engines. They're a set of nested feedback loops that give you feedback and verification of every decision you make as soon as it's economically feasible to do so. This is about as empirical process control kind of rigor as you right. Every decision you make has to run through a gauntlet of feedback loops if you're doing this process carefully and rigorously, but because of that overlap and because of how they feed each other and backfill each other, it doesn't magnify the cost like the old academic software engineering practices did.

The reason I've said all this is to tell you that whereas academic software engineering set off with the wrong goal in mind in the wrong direction, agile set off with the right goal in mind with appropriate disciplines and practices that are well suited to our field. It's not the end game, we have a lot more to go from here, and it's not perfect, but it's a step down the right path for a change, and the past 20 years have really shown some of that. We've got more to go. So, what's next? Well, we've gotten a lot better over the past 20 years at kind of mostly reliably building systems that do what the customers expected them to do. Right, we can mostly do that pretty well. That used to be a very dicey proposition, statistically, in the industry. But sometimes, quite frequently, our systems don't scale as well as they should or they have terrible security problems, and in areas like that, we need to get much better at capacity planning and security design, threat modeling, and things like that. We need to get better at what some people call non-functional requirements. I don't like that term so much, but it describes these things pretty well.

A couple of years ago, I gave this talk, and afterwards, a rather well-known guy in the field came up to me and said, "You know, I like your talk, but man, you know, mature engineering, real engineering, has rules telling you not how not to do things, and at some point, we need to grow up enough to say you just shouldn't do that anymore." And I said, "Well, okay, what do you have in mind?" And he said, "Well, we shouldn't build systems with languages that support mutable state." Okay, so he's a functional programmer, you get that, and you know, I buy that to some degree. But what I didn't say to him was, "Look, your language couldn't have been written unless you use mutable state underneath." But that gets at an important point. At an important point, I think, to make progress, we have to acknowledge the platform-application divide. Some parts of our systems, operating systems, languages, frameworks, are part of the platform. They underlie so many things that it's really important to get them right. It's really important to make them fast, which are often kind of conflicting goals. On top of those platforms, hit applications. And where there are a few platforms, there are a whole bunch of applications, and so we need to have different priorities and different rules for platform kinds of things and for applications that sit on top of them, and use different techniques for this two sides of the boundaries.

So, for example, I do think it's time that we stopped using unsafe languages for applications. Languages that allow null pointer exceptions and buffer overruns and things like that. We have a wealth of safe languages now available, and lately we're getting to see safe languages that also let you program at a very high level of abstraction and also perform really well, which for a long time was, you couldn't have all those things in one bag. And so, we should stop using unsafe languages for applications. I have stopped using mutable state in applications until measurement shows me the need to optimize. I'm not entirely on board with super ultra-pure languages that don't let you have mutable state ever, but I like languages that lead me down that path and make it the path of least resistance, and then give me a loophole I can exploit carefully when I really need to. And for platform things, follow those same rules as much as possible, but you can't build a lot of platforms without breaking those rules, and so in that case, we should try to keep the platforms small and employ formal methods for verification of those.



As you might expect from hearing, you know, this talk to this point, I kind of stayed at arm's length from formal methods. But a couple of years ago, I saw a great talk about formal methods that are being used now to prove the validity and security of small microkernels. And with small enough problems, small enough domains, those formal methods are reaching the point where they're practical to prove that platforms don't have security holes and don't have important bugs that go against their specification and their documentation.

And so, at the platform level, it's still wildly impractical to use that for every application we build, but we can use it for the platforms, and then use safe languages on top of those platforms. And it will take us a long way.

And finally, as every engineering discipline does, keep correcting our course with feedback. Look at what works, look at what doesn't work, think about how to do better next time, try it, write about it, spread the word. As practitioners, we can drive the progress of our field.

Thank you very much for listening. 